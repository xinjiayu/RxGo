package main

import (
	"fmt"
	"math/rand"
	"runtime"
	"sync"
	"sync/atomic"
	"time"

	"github.com/xinjiayu/rxgo"
)

// NetworkCrawler ç½‘ç»œçˆ¬è™«
type NetworkCrawler struct {
	maxConcurrency int
	timeout        time.Duration
}

// NewNetworkCrawler åˆ›å»ºç½‘ç»œçˆ¬è™«
func NewNetworkCrawler() *NetworkCrawler {
	return &NetworkCrawler{
		maxConcurrency: runtime.NumCPU() * 2, // ç½‘ç»œIOå¯ä»¥è®¾ç½®æ›´é«˜çš„å¹¶å‘åº¦
		timeout:        5 * time.Second,
	}
}

// WebPage ç½‘é¡µä¿¡æ¯
type WebPage struct {
	URL      string   `json:"url"`
	Title    string   `json:"title"`
	Size     int      `json:"size"`
	LoadTime int64    `json:"load_time_ms"`
	Status   int      `json:"status"`
	Links    []string `json:"links"`
	Depth    int      `json:"depth"`
}

// CrawlResult çˆ¬å–ç»“æœ
type CrawlResult struct {
	URL         string    `json:"url"`
	Success     bool      `json:"success"`
	StatusCode  int       `json:"status_code"`
	ContentSize int       `json:"content_size"`
	ResponseMS  int64     `json:"response_ms"`
	WorkerID    int       `json:"worker_id"`
	Timestamp   time.Time `json:"timestamp"`
	Error       string    `json:"error,omitempty"`
}

// RunExample è¿è¡Œç½‘ç»œçˆ¬è™«ç¤ºä¾‹
func (nc *NetworkCrawler) RunExample() {
	fmt.Printf("ğŸ•·ï¸ å¯åŠ¨ç½‘ç»œçˆ¬è™« (æœ€å¤§å¹¶å‘: %d)\n", nc.maxConcurrency)

	// 1. ç”Ÿæˆæ¨¡æ‹ŸURLåˆ—è¡¨
	urls := nc.generateURLs(200)
	fmt.Printf("ğŸŒ ç”Ÿæˆäº† %d ä¸ªå¾…çˆ¬å–URL\n", len(urls))

	// 2. å¹¶è¡Œçˆ¬å–
	start := time.Now()
	results := nc.crawlURLsParallel(urls)
	duration := time.Since(start)

	fmt.Printf("âš¡ å¹¶è¡Œçˆ¬å–å®Œæˆ: å¤„ç†äº† %d ä¸ªURLï¼Œè€—æ—¶ %v\n", len(results), duration)

	// 3. ä¸²è¡Œçˆ¬å–å¯¹æ¯”
	// åªå–éƒ¨åˆ†URLè¿›è¡Œä¸²è¡Œå¯¹æ¯”ï¼Œé¿å…è€—æ—¶è¿‡é•¿
	testURLs := urls[:50]
	start = time.Now()
	_ = nc.crawlURLsSerial(testURLs) // ä¸²è¡Œç»“æœä»…ç”¨äºæ€§èƒ½å¯¹æ¯”
	serialDuration := time.Since(start)

	// æŒ‰æ¯”ä¾‹è®¡ç®—é¢„æœŸçš„ä¸²è¡Œæ€»æ—¶é—´
	estimatedSerialTime := time.Duration(float64(serialDuration) * float64(len(urls)) / float64(len(testURLs)))

	fmt.Printf("ğŸŒ ä¸²è¡Œçˆ¬å–æ ·æœ¬å®Œæˆ: %dä¸ªURLè€—æ—¶ %v\n", len(testURLs), serialDuration)
	fmt.Printf("ğŸ“Š é¢„ä¼°ä¸²è¡Œæ€»æ—¶é—´: %v\n", estimatedSerialTime)
	fmt.Printf("ğŸš€ æ€§èƒ½æå‡: %.2fx\n", float64(estimatedSerialTime)/float64(duration))

	// 4. åˆ†æçˆ¬å–ç»“æœ
	nc.analyzeCrawlResults(results)
}

// generateURLs ç”Ÿæˆæ¨¡æ‹ŸURLåˆ—è¡¨
func (nc *NetworkCrawler) generateURLs(count int) []string {
	domains := []string{
		"example.com", "test.org", "demo.net", "sample.io", "mock.dev",
		"api.service", "web.app", "data.site", "news.portal", "blog.platform",
	}

	paths := []string{
		"/", "/home", "/about", "/contact", "/products", "/services",
		"/blog", "/news", "/api/v1/users", "/api/v1/data", "/search",
		"/category/tech", "/category/business", "/archives", "/help", "/docs",
	}

	urls := make([]string, count)
	for i := 0; i < count; i++ {
		domain := domains[rand.Intn(len(domains))]
		path := paths[rand.Intn(len(paths))]
		urls[i] = fmt.Sprintf("https://%s%s?id=%d", domain, path, i+1)
	}

	return urls
}

// crawlURLsParallel å¹¶è¡Œçˆ¬å–URL
func (nc *NetworkCrawler) crawlURLsParallel(urls []string) []CrawlResult {
	var results []CrawlResult
	var mu sync.Mutex
	var wg sync.WaitGroup
	var processedCount int64

	// è½¬æ¢ä¸ºinterface{}åˆ‡ç‰‡
	urlInterfaces := make([]interface{}, len(urls))
	for i, url := range urls {
		urlInterfaces[i] = url
	}

	// åˆ›å»ºå¹¶è¡ŒFlowable
	parallelFlowable := rxgo.ParallelFromSlice(urlInterfaces, nc.maxConcurrency).
		Map(func(v interface{}) (interface{}, error) {
			url := v.(string)
			workerID := int(atomic.AddInt64(&processedCount, 1)) % nc.maxConcurrency

			// çˆ¬å–å•ä¸ªURL
			result := nc.crawlSingleURL(url, workerID)
			return result, nil
		}).
		Filter(func(v interface{}) bool {
			// å¯ä»¥åœ¨è¿™é‡Œè¿‡æ»¤æ‰æŸäº›ç»“æœï¼Œæ¯”å¦‚åªä¿ç•™æˆåŠŸçš„è¯·æ±‚
			return true // ä¿ç•™æ‰€æœ‰ç»“æœç”¨äºåˆ†æ
		})

	// æ”¶é›†ç»“æœ
	subscriptions := parallelFlowable.SubscribeWithCallbacks(
		func(value interface{}) {
			result := value.(CrawlResult)
			mu.Lock()
			results = append(results, result)
			mu.Unlock()
		},
		func(err error) {
			fmt.Printf("âŒ çˆ¬å–é”™è¯¯: %v\n", err)
		},
		func() {
			wg.Done()
		},
	)

	wg.Add(len(subscriptions))

	// è¯·æ±‚æ‰€æœ‰æ•°æ®
	for _, subscription := range subscriptions {
		subscription.Request(int64(len(urls)))
	}

	wg.Wait()
	return results
}

// crawlURLsSerial ä¸²è¡Œçˆ¬å–URL
func (nc *NetworkCrawler) crawlURLsSerial(urls []string) []CrawlResult {
	var results []CrawlResult

	for _, url := range urls {
		result := nc.crawlSingleURL(url, 0)
		results = append(results, result)
	}

	return results
}

// crawlSingleURL çˆ¬å–å•ä¸ªURL
func (nc *NetworkCrawler) crawlSingleURL(url string, workerID int) CrawlResult {
	start := time.Now()

	// æ¨¡æ‹ŸHTTPè¯·æ±‚
	_, statusCode, contentSize, err := nc.simulateHTTPRequest(url)

	result := CrawlResult{
		URL:         url,
		Success:     err == nil,
		StatusCode:  statusCode,
		ContentSize: contentSize,
		ResponseMS:  time.Since(start).Milliseconds(),
		WorkerID:    workerID,
		Timestamp:   time.Now(),
	}

	if err != nil {
		result.Error = err.Error()
	}

	return result
}

// simulateHTTPRequest æ¨¡æ‹ŸHTTPè¯·æ±‚
func (nc *NetworkCrawler) simulateHTTPRequest(url string) (time.Duration, int, int, error) {
	// æ¨¡æ‹Ÿä¸åŒçš„å“åº”æ—¶é—´
	responseTime := time.Duration(rand.Intn(200)+50) * time.Millisecond
	time.Sleep(responseTime)

	// æ¨¡æ‹Ÿä¸åŒçš„çŠ¶æ€ç 
	statusCodes := []int{200, 200, 200, 200, 200, 404, 500, 503} // å¤§éƒ¨åˆ†æˆåŠŸ
	statusCode := statusCodes[rand.Intn(len(statusCodes))]

	// æ¨¡æ‹Ÿä¸åŒçš„å†…å®¹å¤§å°
	contentSize := rand.Intn(100000) + 1000

	// æ¨¡æ‹Ÿé”™è¯¯æƒ…å†µ
	if statusCode >= 400 {
		return responseTime, statusCode, 0, fmt.Errorf("HTTP %d error", statusCode)
	}

	return responseTime, statusCode, contentSize, nil
}

// analyzeCrawlResults åˆ†æçˆ¬å–ç»“æœ
func (nc *NetworkCrawler) analyzeCrawlResults(results []CrawlResult) {
	if len(results) == 0 {
		fmt.Println("âŒ æ²¡æœ‰çˆ¬å–ç»“æœå¯åˆ†æ")
		return
	}

	fmt.Println("\nğŸ“ˆ çˆ¬å–ç»“æœåˆ†æ:")

	// ç»Ÿè®¡ä¿¡æ¯
	var successCount, errorCount int
	var totalResponseTime int64
	var totalContentSize int
	statusStats := make(map[int]int)
	workerStats := make(map[int]struct {
		requests    int
		successes   int
		totalTimeMS int64
		totalSizeKB int
	})

	for _, result := range results {
		if result.Success {
			successCount++
			totalContentSize += result.ContentSize
		} else {
			errorCount++
		}

		totalResponseTime += result.ResponseMS
		statusStats[result.StatusCode]++

		stats := workerStats[result.WorkerID]
		stats.requests++
		if result.Success {
			stats.successes++
			stats.totalSizeKB += result.ContentSize / 1024
		}
		stats.totalTimeMS += result.ResponseMS
		workerStats[result.WorkerID] = stats
	}

	// è¾“å‡ºæ€»ä½“ç»Ÿè®¡
	fmt.Printf("ğŸ“Š æ€»ä½“ç»Ÿè®¡:\n")
	fmt.Printf("   æ€»è¯·æ±‚æ•°: %d\n", len(results))
	fmt.Printf("   æˆåŠŸè¯·æ±‚: %d (%.1f%%)\n", successCount, float64(successCount)*100/float64(len(results)))
	fmt.Printf("   å¤±è´¥è¯·æ±‚: %d (%.1f%%)\n", errorCount, float64(errorCount)*100/float64(len(results)))
	fmt.Printf("   æ€»å†…å®¹å¤§å°: %.2f MB\n", float64(totalContentSize)/(1024*1024))
	fmt.Printf("   å¹³å‡å“åº”æ—¶é—´: %.2f ms\n", float64(totalResponseTime)/float64(len(results)))

	// çŠ¶æ€ç åˆ†å¸ƒ
	fmt.Printf("ğŸ“‹ çŠ¶æ€ç åˆ†å¸ƒ:\n")
	for status, count := range statusStats {
		percentage := float64(count) * 100 / float64(len(results))
		fmt.Printf("   %d: %dæ¬¡ (%.1f%%)\n", status, count, percentage)
	}

	// å·¥ä½œè€…ç»Ÿè®¡
	fmt.Printf("ğŸ”§ å·¥ä½œè€…ç»Ÿè®¡:\n")
	for workerID, stats := range workerStats {
		avgTimePerRequest := float64(stats.totalTimeMS) / float64(stats.requests)
		successRate := float64(stats.successes) * 100 / float64(stats.requests)
		fmt.Printf("   Worker %d: %dè¯·æ±‚, æˆåŠŸç‡%.1f%%, å¹³å‡%.2fms/è¯·æ±‚, æ€»è®¡%.2fMB\n",
			workerID, stats.requests, successRate, avgTimePerRequest, float64(stats.totalSizeKB)/1024)
	}

	// è®¡ç®—ååé‡
	if totalResponseTime > 0 {
		totalTimeSeconds := float64(totalResponseTime) / 1000.0
		throughputPerSecond := float64(len(results)) / totalTimeSeconds * float64(nc.maxConcurrency)
		fmt.Printf("ğŸ“ˆ å¤„ç†ååé‡: %.2f è¯·æ±‚/ç§’\n", throughputPerSecond)
	}
}
