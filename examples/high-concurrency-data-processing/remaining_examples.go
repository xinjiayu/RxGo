package main

import (
	"fmt"
	"math/rand"
	"runtime"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/xinjiayu/rxgo"
)

// ============================================================================
// æ—¥å¿—åˆ†æå™¨
// ============================================================================

// LogAnalytics æ—¥å¿—åˆ†æå™¨
type LogAnalytics struct {
	parallelism int
}

// NewLogAnalytics åˆ›å»ºæ—¥å¿—åˆ†æå™¨
func NewLogAnalytics() *LogAnalytics {
	return &LogAnalytics{
		parallelism: runtime.NumCPU(),
	}
}

// LogEntry æ—¥å¿—æ¡ç›®
type LogEntry struct {
	Timestamp time.Time `json:"timestamp"`
	Level     string    `json:"level"`
	Message   string    `json:"message"`
	Source    string    `json:"source"`
	UserID    string    `json:"user_id,omitempty"`
	IP        string    `json:"ip,omitempty"`
	Size      int       `json:"size"`
}

// LogSummary æ—¥å¿—æ±‡æ€»
type LogSummary struct {
	Level       string    `json:"level"`
	Count       int       `json:"count"`
	TotalSize   int       `json:"total_size"`
	TimeRange   string    `json:"time_range"`
	ProcessedAt time.Time `json:"processed_at"`
	WorkerID    int       `json:"worker_id"`
}

// RunExample è¿è¡Œæ—¥å¿—åˆ†æç¤ºä¾‹
func (la *LogAnalytics) RunExample() {
	fmt.Printf("ğŸ“ˆ å¯åŠ¨æ—¥å¿—åˆ†æå™¨ (å¹¶è¡Œåº¦: %d)\n", la.parallelism)

	// 1. ç”Ÿæˆæ¨¡æ‹Ÿæ—¥å¿—æ•°æ®
	logs := la.generateLogEntries(100000)
	fmt.Printf("ğŸ“‹ ç”Ÿæˆäº† %d æ¡æ—¥å¿—è®°å½•\n", len(logs))

	// 2. å¹¶è¡Œåˆ†ææ—¥å¿—
	start := time.Now()
	summaries := la.analyzeLogsParallel(logs)
	duration := time.Since(start)

	fmt.Printf("âš¡ å¹¶è¡Œæ—¥å¿—åˆ†æå®Œæˆ: ç”Ÿæˆäº† %d ä¸ªæ±‡æ€»ï¼Œè€—æ—¶ %v\n", len(summaries), duration)

	// 3. åˆ†æç»“æœ
	la.analyzeSummaries(summaries)
}

// generateLogEntries ç”Ÿæˆæ—¥å¿—æ¡ç›®
func (la *LogAnalytics) generateLogEntries(count int) []LogEntry {
	levels := []string{"DEBUG", "INFO", "WARN", "ERROR", "FATAL"}
	sources := []string{"web-server", "api-gateway", "database", "cache", "worker", "scheduler"}
	messages := []string{
		"è¯·æ±‚å¤„ç†å®Œæˆ", "æ•°æ®åº“è¿æ¥å»ºç«‹", "ç¼“å­˜å‘½ä¸­", "ç”¨æˆ·ç™»å½•æˆåŠŸ", "æ–‡ä»¶ä¸Šä¼ å®Œæˆ",
		"APIè°ƒç”¨è¶…æ—¶", "å†…å­˜ä½¿ç”¨è¿‡é«˜", "ç£ç›˜ç©ºé—´ä¸è¶³", "ç½‘ç»œè¿æ¥å¼‚å¸¸", "æœåŠ¡å¯åŠ¨",
	}

	logs := make([]LogEntry, count)
	baseTime := time.Now().Add(-24 * time.Hour)

	for i := 0; i < count; i++ {
		logs[i] = LogEntry{
			Timestamp: baseTime.Add(time.Duration(i) * time.Second),
			Level:     levels[rand.Intn(len(levels))],
			Message:   messages[rand.Intn(len(messages))],
			Source:    sources[rand.Intn(len(sources))],
			UserID:    fmt.Sprintf("user_%d", rand.Intn(1000)),
			IP:        fmt.Sprintf("192.168.%d.%d", rand.Intn(255), rand.Intn(255)),
			Size:      rand.Intn(1000) + 100,
		}
	}

	return logs
}

// analyzeLogsParallel å¹¶è¡Œåˆ†ææ—¥å¿—
func (la *LogAnalytics) analyzeLogsParallel(logs []LogEntry) []LogSummary {
	var summaries []LogSummary
	var mu sync.Mutex
	var wg sync.WaitGroup
	var processedBatches int64

	// å°†æ—¥å¿—åˆ†æ‰¹å¤„ç†
	batchSize := 1000
	batches := la.createLogBatches(logs, batchSize)

	// è½¬æ¢ä¸ºinterface{}åˆ‡ç‰‡
	batchInterfaces := make([]interface{}, len(batches))
	for i, batch := range batches {
		batchInterfaces[i] = batch
	}

	// åˆ›å»ºå¹¶è¡ŒFlowable
	parallelFlowable := rxgo.ParallelFromSlice(batchInterfaces, la.parallelism).
		Map(func(v interface{}) (interface{}, error) {
			batch := v.([]LogEntry)
			workerID := int(atomic.AddInt64(&processedBatches, 1)) % la.parallelism

			// åˆ†æå•æ‰¹æ—¥å¿—
			batchSummaries := la.analyzeSingleBatch(batch, workerID)
			return batchSummaries, nil
		}).
		Filter(func(v interface{}) bool {
			summaries := v.([]LogSummary)
			return len(summaries) > 0
		})

	// æ”¶é›†ç»“æœ
	subscriptions := parallelFlowable.SubscribeWithCallbacks(
		func(value interface{}) {
			batchSummaries := value.([]LogSummary)
			mu.Lock()
			summaries = append(summaries, batchSummaries...)
			mu.Unlock()
		},
		func(err error) {
			fmt.Printf("âŒ æ—¥å¿—åˆ†æé”™è¯¯: %v\n", err)
		},
		func() {
			wg.Done()
		},
	)

	wg.Add(len(subscriptions))

	// è¯·æ±‚æ‰€æœ‰æ•°æ®
	for _, subscription := range subscriptions {
		subscription.Request(int64(len(batches)))
	}

	wg.Wait()
	return summaries
}

// createLogBatches åˆ›å»ºæ—¥å¿—æ‰¹æ¬¡
func (la *LogAnalytics) createLogBatches(logs []LogEntry, batchSize int) [][]LogEntry {
	var batches [][]LogEntry
	for i := 0; i < len(logs); i += batchSize {
		end := i + batchSize
		if end > len(logs) {
			end = len(logs)
		}
		batches = append(batches, logs[i:end])
	}
	return batches
}

// analyzeSingleBatch åˆ†æå•æ‰¹æ—¥å¿—
func (la *LogAnalytics) analyzeSingleBatch(logs []LogEntry, workerID int) []LogSummary {
	// æŒ‰çº§åˆ«ç»Ÿè®¡
	levelStats := make(map[string]struct {
		count     int
		totalSize int
		minTime   time.Time
		maxTime   time.Time
	})

	for _, log := range logs {
		stats := levelStats[log.Level]
		stats.count++
		stats.totalSize += log.Size

		if stats.minTime.IsZero() || log.Timestamp.Before(stats.minTime) {
			stats.minTime = log.Timestamp
		}
		if stats.maxTime.IsZero() || log.Timestamp.After(stats.maxTime) {
			stats.maxTime = log.Timestamp
		}

		levelStats[log.Level] = stats
	}

	// è½¬æ¢ä¸ºæ±‡æ€»
	var summaries []LogSummary
	for level, stats := range levelStats {
		timeRange := fmt.Sprintf("%s ~ %s",
			stats.minTime.Format("15:04:05"),
			stats.maxTime.Format("15:04:05"))

		summaries = append(summaries, LogSummary{
			Level:       level,
			Count:       stats.count,
			TotalSize:   stats.totalSize,
			TimeRange:   timeRange,
			ProcessedAt: time.Now(),
			WorkerID:    workerID,
		})
	}

	return summaries
}

// analyzeSummaries åˆ†ææ±‡æ€»ç»“æœ
func (la *LogAnalytics) analyzeSummaries(summaries []LogSummary) {
	fmt.Println("\nğŸ“ˆ æ—¥å¿—åˆ†æç»“æœ:")

	// æŒ‰çº§åˆ«èšåˆ
	levelTotals := make(map[string]struct {
		count     int
		totalSize int
	})

	for _, summary := range summaries {
		totals := levelTotals[summary.Level]
		totals.count += summary.Count
		totals.totalSize += summary.TotalSize
		levelTotals[summary.Level] = totals
	}

	// è¾“å‡ºç»“æœ
	fmt.Printf("ğŸ“Š æŒ‰æ—¥å¿—çº§åˆ«ç»Ÿè®¡:\n")
	totalLogs := 0
	for level, totals := range levelTotals {
		fmt.Printf("   %s: %dæ¡æ—¥å¿—, %.2fKB\n", level, totals.count, float64(totals.totalSize)/1024)
		totalLogs += totals.count
	}
	fmt.Printf("   æ€»è®¡: %dæ¡æ—¥å¿—\n", totalLogs)
}

// ============================================================================
// æµå¤„ç†å™¨
// ============================================================================

// StreamProcessor æµå¤„ç†å™¨
type StreamProcessor struct {
	parallelism int
}

// NewStreamProcessor åˆ›å»ºæµå¤„ç†å™¨
func NewStreamProcessor() *StreamProcessor {
	return &StreamProcessor{
		parallelism: runtime.NumCPU(),
	}
}

// RunExample è¿è¡Œæµå¤„ç†ç¤ºä¾‹
func (sp *StreamProcessor) RunExample() {
	fmt.Printf("ğŸŒŠ å¯åŠ¨æµå¤„ç†å™¨ (å¹¶è¡Œåº¦: %d)\n", sp.parallelism)

	// æ¨¡æ‹Ÿå®æ—¶æ•°æ®æµå¤„ç†
	fmt.Println("ğŸ“¡ æ¨¡æ‹Ÿå®æ—¶æ•°æ®æµå¤„ç†...")

	// åˆ›å»ºæ•°æ®æµ
	dataStream := sp.createDataStream(1000)

	// å¹¶è¡Œå¤„ç†æµæ•°æ®
	start := time.Now()
	results := sp.processStreamParallel(dataStream)
	duration := time.Since(start)

	fmt.Printf("âš¡ æµå¤„ç†å®Œæˆ: å¤„ç†äº† %d ä¸ªæ•°æ®åŒ…ï¼Œè€—æ—¶ %v\n", len(results), duration)
	fmt.Printf("ğŸ“ˆ å¤„ç†é€Ÿåº¦: %.2f åŒ…/ç§’\n", float64(len(results))/duration.Seconds())
}

// createDataStream åˆ›å»ºæ•°æ®æµ
func (sp *StreamProcessor) createDataStream(count int) []interface{} {
	stream := make([]interface{}, count)
	for i := 0; i < count; i++ {
		stream[i] = map[string]interface{}{
			"id":        i + 1,
			"timestamp": time.Now(),
			"value":     rand.Float64() * 100,
			"category":  fmt.Sprintf("cat_%d", rand.Intn(5)),
		}
	}
	return stream
}

// processStreamParallel å¹¶è¡Œå¤„ç†æµæ•°æ®
func (sp *StreamProcessor) processStreamParallel(stream []interface{}) []interface{} {
	var results []interface{}
	var mu sync.Mutex
	var wg sync.WaitGroup

	// åˆ›å»ºå¹¶è¡ŒFlowable
	parallelFlowable := rxgo.ParallelFromSlice(stream, sp.parallelism).
		Map(func(v interface{}) (interface{}, error) {
			data := v.(map[string]interface{})

			// æ¨¡æ‹Ÿæµæ•°æ®å¤„ç†
			processed := map[string]interface{}{
				"id":              data["id"],
				"original_value":  data["value"],
				"processed_value": data["value"].(float64) * 2,
				"category":        data["category"],
				"processed_at":    time.Now(),
			}

			// æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
			time.Sleep(time.Microsecond * 50)

			return processed, nil
		})

	// æ”¶é›†ç»“æœ
	subscriptions := parallelFlowable.SubscribeWithCallbacks(
		func(value interface{}) {
			mu.Lock()
			results = append(results, value)
			mu.Unlock()
		},
		func(err error) {
			fmt.Printf("âŒ æµå¤„ç†é”™è¯¯: %v\n", err)
		},
		func() {
			wg.Done()
		},
	)

	wg.Add(len(subscriptions))

	// è¯·æ±‚æ‰€æœ‰æ•°æ®
	for _, subscription := range subscriptions {
		subscription.Request(int64(len(stream)))
	}

	wg.Wait()
	return results
}

// ============================================================================
// MapReduceå¤„ç†å™¨
// ============================================================================

// MapReduceProcessor MapReduceå¤„ç†å™¨
type MapReduceProcessor struct {
	parallelism int
}

// NewMapReduceProcessor åˆ›å»ºMapReduceå¤„ç†å™¨
func NewMapReduceProcessor() *MapReduceProcessor {
	return &MapReduceProcessor{
		parallelism: runtime.NumCPU(),
	}
}

// RunExample è¿è¡ŒMapReduceç¤ºä¾‹
func (mrp *MapReduceProcessor) RunExample() {
	fmt.Printf("ğŸ—ºï¸ å¯åŠ¨MapReduceå¤„ç†å™¨ (å¹¶è¡Œåº¦: %d)\n", mrp.parallelism)

	// ç¤ºä¾‹ï¼šè¯é¢‘ç»Ÿè®¡
	fmt.Println("ğŸ“„ æ‰§è¡Œå¤§è§„æ¨¡æ–‡æœ¬è¯é¢‘ç»Ÿè®¡...")

	// ç”Ÿæˆæ¨¡æ‹Ÿæ–‡æ¡£
	documents := mrp.generateDocuments(1000)
	fmt.Printf("ğŸ“š ç”Ÿæˆäº† %d ä¸ªæ–‡æ¡£\n", len(documents))

	// æ‰§è¡ŒMapReduce
	start := time.Now()
	wordCounts := mrp.wordCountMapReduce(documents)
	duration := time.Since(start)

	fmt.Printf("âš¡ MapReduceå®Œæˆ: ç»Ÿè®¡äº† %d ä¸ªå”¯ä¸€è¯æ±‡ï¼Œè€—æ—¶ %v\n", len(wordCounts), duration)

	// è¾“å‡ºå‰10ä¸ªæœ€é¢‘ç¹çš„è¯
	mrp.showTopWords(wordCounts, 10)
}

// generateDocuments ç”Ÿæˆæ¨¡æ‹Ÿæ–‡æ¡£
func (mrp *MapReduceProcessor) generateDocuments(count int) []string {
	words := []string{
		"golang", "programming", "concurrent", "parallel", "reactive", "stream",
		"data", "processing", "algorithm", "performance", "optimization", "scalable",
		"distributed", "system", "architecture", "microservice", "cloud", "computing",
	}

	documents := make([]string, count)
	for i := 0; i < count; i++ {
		// æ¯ä¸ªæ–‡æ¡£åŒ…å«éšæœºæ•°é‡çš„éšæœºè¯æ±‡
		docLength := rand.Intn(50) + 10
		var docWords []string
		for j := 0; j < docLength; j++ {
			docWords = append(docWords, words[rand.Intn(len(words))])
		}
		documents[i] = strings.Join(docWords, " ")
	}

	return documents
}

// wordCountMapReduce æ‰§è¡Œè¯é¢‘ç»Ÿè®¡MapReduce
func (mrp *MapReduceProcessor) wordCountMapReduce(documents []string) map[string]int {
	// è½¬æ¢ä¸ºinterface{}åˆ‡ç‰‡
	docInterfaces := make([]interface{}, len(documents))
	for i, doc := range documents {
		docInterfaces[i] = doc
	}

	// Mapé˜¶æ®µï¼šå¹¶è¡Œå¤„ç†æ¯ä¸ªæ–‡æ¡£ï¼Œè¾“å‡ºè¯æ±‡è®¡æ•°
	var allWordCounts []map[string]int
	var mu sync.Mutex
	var wg sync.WaitGroup

	parallelFlowable := rxgo.ParallelFromSlice(docInterfaces, mrp.parallelism).
		Map(func(v interface{}) (interface{}, error) {
			document := v.(string)

			// Mapé˜¶æ®µï¼šç»Ÿè®¡å•ä¸ªæ–‡æ¡£çš„è¯é¢‘
			wordCount := make(map[string]int)
			words := strings.Fields(document)
			for _, word := range words {
				word = strings.ToLower(strings.Trim(word, ".,!?;"))
				if len(word) > 0 {
					wordCount[word]++
				}
			}

			return wordCount, nil
		})

	// æ”¶é›†Mapç»“æœ
	subscriptions := parallelFlowable.SubscribeWithCallbacks(
		func(value interface{}) {
			wordCount := value.(map[string]int)
			mu.Lock()
			allWordCounts = append(allWordCounts, wordCount)
			mu.Unlock()
		},
		func(err error) {
			fmt.Printf("âŒ Mapé˜¶æ®µé”™è¯¯: %v\n", err)
		},
		func() {
			wg.Done()
		},
	)

	wg.Add(len(subscriptions))

	// è¯·æ±‚æ‰€æœ‰æ•°æ®
	for _, subscription := range subscriptions {
		subscription.Request(int64(len(documents)))
	}

	wg.Wait()

	// Reduceé˜¶æ®µï¼šåˆå¹¶æ‰€æœ‰è¯é¢‘ç»Ÿè®¡
	finalWordCount := make(map[string]int)
	for _, wordCount := range allWordCounts {
		for word, count := range wordCount {
			finalWordCount[word] += count
		}
	}

	return finalWordCount
}

// showTopWords æ˜¾ç¤ºæœ€é¢‘ç¹çš„è¯æ±‡
func (mrp *MapReduceProcessor) showTopWords(wordCounts map[string]int, topN int) {
	// è½¬æ¢ä¸ºåˆ‡ç‰‡å¹¶æ’åº
	type wordFreq struct {
		word  string
		count int
	}

	var frequencies []wordFreq
	for word, count := range wordCounts {
		frequencies = append(frequencies, wordFreq{word, count})
	}

	// ç®€å•çš„å†’æ³¡æ’åºï¼ˆæ¼”ç¤ºç”¨ï¼‰
	for i := 0; i < len(frequencies)-1; i++ {
		for j := 0; j < len(frequencies)-i-1; j++ {
			if frequencies[j].count < frequencies[j+1].count {
				frequencies[j], frequencies[j+1] = frequencies[j+1], frequencies[j]
			}
		}
	}

	fmt.Printf("ğŸ“Š å‰ %d ä¸ªæœ€é¢‘ç¹çš„è¯æ±‡:\n", topN)
	for i := 0; i < topN && i < len(frequencies); i++ {
		freq := frequencies[i]
		fmt.Printf("   %d. %s: %dæ¬¡\n", i+1, freq.word, freq.count)
	}
}
